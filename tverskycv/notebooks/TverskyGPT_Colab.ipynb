{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TverskyGPT: Parameter-Efficient GPT with Shared Feature Bank\n",
    "\n",
    "This notebook demonstrates the TverskyGPT model, which uses Tversky similarity-based attention with a shared feature bank to significantly reduce parameter count compared to standard GPT models.\n",
    "\n",
    "<a href=\"https://colab.research.google.com/github/YOUR_USERNAME/tversky-similarity-grad/blob/main/tverskycv/notebooks/TverskyGPT_Colab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Installation\n",
    "\n",
    "First, let's install the required dependencies and clone/setup the repository.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "%pip install torch torchvision transformers numpy matplotlib seaborn tqdm\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Import Libraries\n",
    "\n",
    "Import the necessary modules for working with TverskyGPT.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from transformers import GPT2Config\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# Setup for Google Colab\n",
    "# Uncomment the following lines if running in Colab:\n",
    "# %cd /content\n",
    "# !git clone https://github.com/YOUR_USERNAME/tversky-similarity-grad.git\n",
    "# %cd tversky-similarity-grad\n",
    "# sys.path.insert(0, '/content/tversky-similarity-grad')\n",
    "\n",
    "# For local development, uncomment and adjust:\n",
    "# sys.path.insert(0, os.path.abspath('../..'))\n",
    "\n",
    "# Import TverskyGPT utilities\n",
    "try:\n",
    "    from tverskycv.models.backbones.tversky_gpt import (\n",
    "        create_tversky_gpt_from_config,\n",
    "        TverskyGPTModel,\n",
    "        count_parameters,\n",
    "        get_shared_parameter_info\n",
    "    )\n",
    "    print(\"\u2713 Imports successful!\")\n",
    "except ImportError as e:\n",
    "    print(f\"Import error: {e}\")\n",
    "    print(\"Make sure you've cloned the repository and added it to the Python path.\")\n",
    "    print(\"In Colab, run: !git clone https://github.com/YOUR_USERNAME/tversky-similarity-grad.git\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Understanding TverskyGPT\n",
    "\n",
    "TverskyGPT uses **Tversky similarity-based attention** instead of standard dot-product attention. The key innovation is the **GlobalFeature bank**, which allows sharing feature matrices and Tversky parameters across layers, dramatically reducing parameter count.\n",
    "\n",
    "### Key Benefits:\n",
    "- **Parameter Reduction**: Shared feature matrices reduce total parameters\n",
    "- **Memory Efficient**: Less memory usage during training and inference\n",
    "- **GPT-2 Compatible**: Uses same config structure as GPT-2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Create a TverskyGPT Model\n",
    "\n",
    "Let's create a small TverskyGPT model to demonstrate the functionality.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a GPT2Config (small model for demonstration)\n",
    "config = GPT2Config(\n",
    "    vocab_size=50257,      # Standard GPT-2 vocabulary size\n",
    "    n_embed=256,           # Embedding dimension (smaller for demo)\n",
    "    n_layer=6,             # Number of transformer layers\n",
    "    n_head=8,              # Number of attention heads\n",
    "    n_positions=1024,      # Maximum sequence length\n",
    "    n_inner=1024,          # FFN inner dimension\n",
    "    resid_pdrop=0.1,       # Residual dropout\n",
    "    embd_pdrop=0.1,        # Embedding dropout\n",
    "    layer_norm_epsilon=1e-5,\n",
    ")\n",
    "\n",
    "print(\"Configuration:\")\n",
    "print(f\"  Vocab size: {config.vocab_size:,}\")\n",
    "print(f\"  Embedding dim: {config.n_embed}\")\n",
    "print(f\"  Number of layers: {config.n_layer}\")\n",
    "print(f\"  Number of heads: {config.n_head}\")\n",
    "print(f\"  Max sequence length: {config.n_positions}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create model with shared parameters (maximizes parameter reduction)\n",
    "model = create_tversky_gpt_from_config(\n",
    "    config=config,\n",
    "    feature_key='shared',\n",
    "    alpha=0.5,\n",
    "    beta=0.5,\n",
    "    gamma=1.0,\n",
    "    share_across_layers=True,  # All layers share the same feature_key\n",
    ")\n",
    "\n",
    "print(f\"\u2713 Model created!\")\n",
    "print(f\"  Total parameters: {count_parameters(model):,}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Analyze Shared Parameters\n",
    "\n",
    "Let's examine how the GlobalFeature bank reduces parameters by sharing features across layers.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get information about shared parameters\n",
    "info = get_shared_parameter_info(model)\n",
    "\n",
    "print(\"Shared Parameter Analysis:\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Total model parameters: {info['total_model_params']:,}\")\n",
    "print(f\"Total shared parameters: {info['total_shared_params']:,}\")\n",
    "print(f\"Shared percentage: {info['shared_percentage']:.2f}%\")\n",
    "print(\"\\nShared feature matrices:\")\n",
    "for key, count in info['shared_parameters'].items():\n",
    "    print(f\"  {key}: {count:,} parameters\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Compare Shared vs Non-Shared Models\n",
    "\n",
    "Let's compare models with and without parameter sharing to see the reduction.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare Shared vs Non-Shared Models\n",
    "# Clear GlobalFeature bank to ensure fair comparison\n",
    "from tverskycv.models.backbones.shared_tversky import GlobalFeature\n",
    "\n",
    "# Clear the GlobalFeature bank before creating models for accurate comparison\n",
    "gf = GlobalFeature()\n",
    "gf.clear()\n",
    "\n",
    "# Model with shared parameters (maximizes reduction)\n",
    "model_shared = create_tversky_gpt_from_config(\n",
    "    config=config,\n",
    "    feature_key='shared_comparison',\n",
    "    share_across_layers=True,\n",
    ")\n",
    "\n",
    "# Clear again before creating non-shared model\n",
    "# (This ensures non-shared model doesn't reuse features from shared model)\n",
    "gf.clear()\n",
    "\n",
    "# Model without shared parameters (each layer has own features)\n",
    "model_not_shared = create_tversky_gpt_from_config(\n",
    "    config=config,\n",
    "    feature_key='non_shared_comparison',\n",
    "    share_across_layers=False,\n",
    ")\n",
    "\n",
    "params_shared = count_parameters(model_shared)\n",
    "params_not_shared = count_parameters(model_not_shared)\n",
    "reduction = ((params_not_shared - params_shared) / params_not_shared) * 100\n",
    "\n",
    "print(\"Parameter Comparison:\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Model with shared features:    {params_shared:,} parameters\")\n",
    "print(f\"Model without shared features: {params_not_shared:,} parameters\")\n",
    "print(f\"Parameter reduction:           {reduction:.2f}%\")\n",
    "print(f\"Parameters saved:              {params_not_shared - params_shared:,}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the comparison\n",
    "fig, ax = plt.subplots(1, 1, figsize=(10, 6))\n",
    "\n",
    "models = ['Shared\\nFeatures', 'Non-Shared\\nFeatures']\n",
    "params = [params_shared, params_not_shared]\n",
    "colors = ['#2ecc71', '#e74c3c']\n",
    "\n",
    "bars = ax.bar(models, params, color=colors, alpha=0.7, edgecolor='black', linewidth=2)\n",
    "ax.set_ylabel('Number of Parameters', fontsize=12, fontweight='bold')\n",
    "ax.set_title('Parameter Count: Shared vs Non-Shared TverskyGPT', fontsize=14, fontweight='bold')\n",
    "ax.grid(axis='y', alpha=0.3, linestyle='--')\n",
    "\n",
    "# Add value labels on bars\n",
    "for bar, param in zip(bars, params):\n",
    "    height = bar.get_height()\n",
    "    ax.text(bar.get_x() + bar.get_width()/2., height,\n",
    "            f'{param:,}\\n({param/1e6:.2f}M)',\n",
    "            ha='center', va='bottom', fontsize=11, fontweight='bold')\n",
    "\n",
    "# Add reduction annotation\n",
    "ax.annotate(f'Reduction: {reduction:.1f}%',\n",
    "            xy=(0.5, 0.5), xytext=(0.5, max(params) * 0.7),\n",
    "            arrowprops=dict(arrowstyle='->', color='black', lw=2),\n",
    "            fontsize=12, fontweight='bold', ha='center',\n",
    "            bbox=dict(boxstyle='round,pad=0.5', facecolor='yellow', alpha=0.7))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Forward Pass Example\n",
    "\n",
    "Let's test the model with a forward pass.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set model to evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# Create dummy input tokens\n",
    "batch_size = 2\n",
    "seq_length = 10\n",
    "input_ids = torch.randint(0, config.vocab_size, (batch_size, seq_length))\n",
    "\n",
    "print(f\"Input shape: {input_ids.shape}\")\n",
    "print(f\"Input tokens (first batch): {input_ids[0].tolist()}\")\n",
    "\n",
    "# Forward pass\n",
    "with torch.no_grad():\n",
    "    outputs = model(input_ids=input_ids)\n",
    "    hidden_states = outputs.last_hidden_state\n",
    "\n",
    "print(f\"\\nOutput shape: {hidden_states.shape}\")\n",
    "print(f\"Expected: ({batch_size}, {seq_length}, {config.n_embed})\")\n",
    "print(f\"\u2713 Forward pass successful!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Scaling Analysis\n",
    "\n",
    "Let's see how parameter reduction scales with different model sizes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test different model sizes\n",
    "from tverskycv.models.backbones.shared_tversky import GlobalFeature\n",
    "\n",
    "model_configs = [\n",
    "    {\"n_embed\": 128, \"n_layer\": 4, \"n_head\": 4, \"name\": \"Small\"},\n",
    "    {\"n_embed\": 256, \"n_layer\": 6, \"n_head\": 8, \"name\": \"Medium\"},\n",
    "    {\"n_embed\": 512, \"n_layer\": 8, \"n_head\": 8, \"name\": \"Large\"},\n",
    "]\n",
    "\n",
    "results = []\n",
    "gf = GlobalFeature()\n",
    "\n",
    "for model_cfg in model_configs:\n",
    "    # Clear GlobalFeature bank for each model size comparison\n",
    "    gf.clear()\n",
    "    \n",
    "    test_config = GPT2Config(\n",
    "        vocab_size=50257,\n",
    "        n_embed=model_cfg[\"n_embed\"],\n",
    "        n_layer=model_cfg[\"n_layer\"],\n",
    "        n_head=model_cfg[\"n_head\"],\n",
    "        n_positions=1024,\n",
    "        n_inner=model_cfg[\"n_embed\"] * 4,\n",
    "        resid_pdrop=0.1,\n",
    "        embd_pdrop=0.1,\n",
    "    )\n",
    "    \n",
    "    # Create shared model\n",
    "    model_s = create_tversky_gpt_from_config(\n",
    "        test_config, \n",
    "        feature_key=f\"shared_{model_cfg['name'].lower()}\",\n",
    "        share_across_layers=True\n",
    "    )\n",
    "    \n",
    "    # Clear before creating non-shared model\n",
    "    gf.clear()\n",
    "    \n",
    "    # Create non-shared model\n",
    "    model_ns = create_tversky_gpt_from_config(\n",
    "        test_config, \n",
    "        feature_key=f\"non_shared_{model_cfg['name'].lower()}\",\n",
    "        share_across_layers=False\n",
    "    )\n",
    "    \n",
    "    params_s = count_parameters(model_s)\n",
    "    params_ns = count_parameters(model_ns)\n",
    "    reduction = ((params_ns - params_s) / params_ns) * 100\n",
    "    \n",
    "    results.append({\n",
    "        \"name\": model_cfg[\"name\"],\n",
    "        \"shared\": params_s,\n",
    "        \"non_shared\": params_ns,\n",
    "        \"reduction\": reduction\n",
    "    })\n",
    "\n",
    "# Display results\n",
    "print(\"Scaling Analysis:\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"{'Model':<10} {'Shared (M)':<15} {'Non-Shared (M)':<18} {'Reduction %':<15} {'Saved (M)':<15}\")\n",
    "print(\"-\" * 80)\n",
    "for r in results:\n",
    "    print(f\"{r['name']:<10} {r['shared']/1e6:<15.2f} {r['non_shared']/1e6:<18.2f} \"\n",
    "          f\"{r['reduction']:<15.2f} {(r['non_shared']-r['shared'])/1e6:<15.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize scaling results\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "# Plot 1: Parameter count comparison\n",
    "ax1 = axes[0]\n",
    "x = np.arange(len(results))\n",
    "width = 0.35\n",
    "\n",
    "shared_params = [r['shared']/1e6 for r in results]\n",
    "non_shared_params = [r['non_shared']/1e6 for r in results]\n",
    "\n",
    "bars1 = ax1.bar(x - width/2, shared_params, width, label='Shared Features', color='#2ecc71', alpha=0.7)\n",
    "bars2 = ax1.bar(x + width/2, non_shared_params, width, label='Non-Shared Features', color='#e74c3c', alpha=0.7)\n",
    "\n",
    "ax1.set_xlabel('Model Size', fontsize=12, fontweight='bold')\n",
    "ax1.set_ylabel('Parameters (Millions)', fontsize=12, fontweight='bold')\n",
    "ax1.set_title('Parameter Count by Model Size', fontsize=14, fontweight='bold')\n",
    "ax1.set_xticks(x)\n",
    "ax1.set_xticklabels([r['name'] for r in results])\n",
    "ax1.legend()\n",
    "ax1.grid(axis='y', alpha=0.3, linestyle='--')\n",
    "\n",
    "# Add value labels\n",
    "for bars in [bars1, bars2]:\n",
    "    for bar in bars:\n",
    "        height = bar.get_height()\n",
    "        ax1.text(bar.get_x() + bar.get_width()/2., height,\n",
    "                f'{height:.2f}M',\n",
    "                ha='center', va='bottom', fontsize=9)\n",
    "\n",
    "# Plot 2: Reduction percentage\n",
    "ax2 = axes[1]\n",
    "reductions = [r['reduction'] for r in results]\n",
    "bars3 = ax2.bar([r['name'] for r in results], reductions, color='#3498db', alpha=0.7, edgecolor='black', linewidth=2)\n",
    "\n",
    "ax2.set_xlabel('Model Size', fontsize=12, fontweight='bold')\n",
    "ax2.set_ylabel('Parameter Reduction (%)', fontsize=12, fontweight='bold')\n",
    "ax2.set_title('Parameter Reduction by Model Size', fontsize=14, fontweight='bold')\n",
    "ax2.grid(axis='y', alpha=0.3, linestyle='--')\n",
    "\n",
    "# Add value labels\n",
    "for bar, reduction in zip(bars3, reductions):\n",
    "    height = bar.get_height()\n",
    "    ax2.text(bar.get_x() + bar.get_width()/2., height,\n",
    "            f'{reduction:.2f}%',\n",
    "            ha='center', va='bottom', fontsize=11, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Memory Usage Comparison\n",
    "\n",
    "Let's compare the memory footprint of shared vs non-shared models.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Estimate memory usage (in MB)\n",
    "# Each float32 parameter uses 4 bytes\n",
    "def estimate_memory_mb(model):\n",
    "    params = count_parameters(model)\n",
    "    # Parameters + optimizer states (Adam uses ~2x for momentum and variance)\n",
    "    # For inference, we only count parameters\n",
    "    return (params * 4) / (1024 * 1024)\n",
    "\n",
    "memory_shared = estimate_memory_mb(model_shared)\n",
    "memory_not_shared = estimate_memory_mb(model_not_shared)\n",
    "memory_saved = memory_not_shared - memory_shared\n",
    "\n",
    "print(\"Memory Usage (Inference):\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Shared model:      {memory_shared:.2f} MB\")\n",
    "print(f\"Non-shared model:  {memory_not_shared:.2f} MB\")\n",
    "print(f\"Memory saved:       {memory_saved:.2f} MB ({memory_saved/memory_not_shared*100:.2f}%)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Custom Configuration Example\n",
    "\n",
    "You can customize the Tversky parameters (alpha, beta, gamma) to control the similarity function behavior.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create model with custom Tversky parameters\n",
    "custom_model = create_tversky_gpt_from_config(\n",
    "    config=config,\n",
    "    feature_key='custom',\n",
    "    alpha=0.7,   # Higher alpha = more weight on false positives\n",
    "    beta=0.3,   # Lower beta = less weight on false negatives\n",
    "    gamma=1.5,  # Higher gamma = more weight on common features\n",
    "    share_across_layers=True,\n",
    ")\n",
    "\n",
    "print(\"Custom TverskyGPT Model:\")\n",
    "print(f\"  Alpha: 0.7 (controls false positives)\")\n",
    "print(f\"  Beta:  0.3 (controls false negatives)\")\n",
    "print(f\"  Gamma: 1.5 (controls common features)\")\n",
    "print(f\"  Parameters: {count_parameters(custom_model):,}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Summary\n",
    "\n",
    "### Key Takeaways:\n",
    "\n",
    "1. **Parameter Reduction**: TverskyGPT with shared features significantly reduces parameters compared to non-shared models\n",
    "2. **Memory Efficiency**: Lower memory footprint for both training and inference\n",
    "3. **Flexibility**: Can customize Tversky parameters (alpha, beta, gamma) for different use cases\n",
    "4. **GPT-2 Compatible**: Uses standard GPT2Config, making it easy to integrate\n",
    "\n",
    "### Use Cases:\n",
    "\n",
    "- **Resource-Constrained Environments**: When memory/compute is limited\n",
    "- **Large-Scale Models**: Parameter reduction becomes more significant with larger models\n",
    "- **Research**: Experimenting with Tversky similarity-based attention mechanisms\n",
    "\n",
    "### Next Steps:\n",
    "\n",
    "- Try different model sizes and configurations\n",
    "- Experiment with different alpha, beta, gamma values\n",
    "- Compare performance with standard GPT-2 models\n",
    "- Fine-tune on your specific task\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}