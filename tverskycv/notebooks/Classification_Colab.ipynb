{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fruits-360 Classification with TverskyReduceBackbone\n",
    "\n",
    "This notebook demonstrates parameter-efficient fruit classification using **TverskyReduceBackbone** with the **GlobalFeature bank** for feature sharing. The model combines CNN feature extraction with Tversky similarity-based projections that share feature matrices across layers, significantly reducing the number of trainable parameters.\n",
    "\n",
    "<a href=\"https://colab.research.google.com/github/madch3m/tverskysimilaritygrad/blob/main/tverskycv/notebooks/Classification_Colab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Installation\n",
    "\n",
    "Install required packages and set up the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clone the repository (for Colab users)\n",
    "# \n",
    "# INSTRUCTIONS FOR COLAB:\n",
    "# 1. Uncomment the git clone line below\n",
    "# 2. The repository will be cloned to /content/tverskysimilaritygrad\n",
    "# 3. Run this cell to clone the repository\n",
    "# 4. The import cell will automatically detect and use the cloned repository\n",
    "\n",
    "# Uncomment the line below:\n",
    "# !git clone https://github.com/madch3m/tverskysimilaritygrad.git\n",
    "\n",
    "# For local development, skip this cell\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# Check if we're in Colab\n",
    "IN_COLAB = os.path.exists('/content')\n",
    "\n",
    "if IN_COLAB:\n",
    "    repo_path = '/content/tverskysimilaritygrad'\n",
    "    if os.path.exists(repo_path):\n",
    "        print(f\"\u2713 Repository found at {repo_path}\")\n",
    "        os.chdir(repo_path)\n",
    "        # Add to sys.path immediately\n",
    "        if repo_path not in sys.path:\n",
    "            sys.path.insert(0, repo_path)\n",
    "        print(f\"\u2713 Changed to: {os.getcwd()}\")\n",
    "        print(f\"\u2713 Added to sys.path\")\n",
    "        \n",
    "        # Verify tverskycv exists\n",
    "        if os.path.exists(os.path.join(repo_path, 'tverskycv')):\n",
    "            print(f\"\u2713 tverskycv folder found\")\n",
    "        else:\n",
    "            print(f\"\u26a0 tverskycv folder not found at {repo_path}/tverskycv\")\n",
    "    else:\n",
    "        print(\"\u26a0 Repository not found.\")\n",
    "        print(\"  Please uncomment and run the git clone command above.\")\n",
    "        print(f\"  Expected location: {repo_path}\")\n",
    "else:\n",
    "    print(\"\u2713 Running locally - repository should already be available\")\n",
    "    # For local, try to find project root\n",
    "    current = os.getcwd()\n",
    "    if os.path.exists(os.path.join(current, 'tverskycv')):\n",
    "        print(f\"\u2713 Found tverskycv in current directory: {current}\")\n",
    "    elif os.path.exists(os.path.join(current, '..', 'tverskycv')):\n",
    "        parent = os.path.abspath('..')\n",
    "        print(f\"\u2713 Found tverskycv in parent directory: {parent}\")\n",
    "        if parent not in sys.path:\n",
    "            sys.path.insert(0, parent)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "%pip install torch torchvision transformers numpy matplotlib seaborn tqdm datasets Pillow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Import Libraries and Setup Path\n",
    "\n",
    "Import necessary modules and set up the project path."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Libraries and Setup Path\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "from datasets import load_dataset\n",
    "import sys\n",
    "import os\n",
    "from collections import Counter\n",
    "\n",
    "# Automatic path setup for both Colab and local development\n",
    "print(\"Setting up paths...\")\n",
    "_current_dir = os.getcwd()\n",
    "print(f\"Current directory: {_current_dir}\")\n",
    "\n",
    "_project_root = None\n",
    "\n",
    "# Try to find project root (directory containing tverskycv folder)\n",
    "# Check multiple possible locations\n",
    "_search_paths = [\n",
    "    _current_dir,  # Current directory\n",
    "    os.path.join(_current_dir, 'tverskysimilaritygrad'),  # If we're in /content\n",
    "    '/content/tverskysimilaritygrad',  # Colab default after clone\n",
    "    os.path.join(_current_dir, '..'),\n",
    "    os.path.join(_current_dir, '..', '..'),\n",
    "    os.path.join(_current_dir, '..', '..', '..'),\n",
    "    os.path.abspath('.'),  # Absolute current\n",
    "    os.path.abspath('..'),  # Parent\n",
    "]\n",
    "\n",
    "print(\"\\nSearching for project root...\")\n",
    "for path in _search_paths:\n",
    "    abs_path = os.path.abspath(path)\n",
    "    tverskycv_path = os.path.join(abs_path, 'tverskycv')\n",
    "    if os.path.exists(tverskycv_path) and os.path.isdir(tverskycv_path):\n",
    "        _project_root = abs_path\n",
    "        print(f\"  \u2713 Found tverskycv at: {tverskycv_path}\")\n",
    "        break\n",
    "    else:\n",
    "        print(f\"  \u2717 Not found: {tverskycv_path}\")\n",
    "\n",
    "# Change to project root if found\n",
    "if _project_root:\n",
    "    os.chdir(_project_root)\n",
    "    if _project_root not in sys.path:\n",
    "        sys.path.insert(0, _project_root)\n",
    "    print(f\"\\n\u2713 Project root: {_project_root}\")\n",
    "    print(f\"\u2713 Working directory: {os.getcwd()}\")\n",
    "    print(f\"\u2713 Added to sys.path\")\n",
    "else:\n",
    "    # Fallback: try adding current directory and common paths\n",
    "    print(\"\\n\u26a0 Project root not found. Trying fallback paths...\")\n",
    "    _fallback_paths = [\n",
    "        os.path.abspath('.'),\n",
    "        os.path.abspath('..'),\n",
    "        os.path.abspath('../..'),\n",
    "        '/content',\n",
    "        '/content/tverskysimilaritygrad',\n",
    "    ]\n",
    "    for path in _fallback_paths:\n",
    "        abs_path = os.path.abspath(path)\n",
    "        if abs_path not in sys.path and os.path.exists(abs_path):\n",
    "            sys.path.insert(0, abs_path)\n",
    "            print(f\"  Added to sys.path: {abs_path}\")\n",
    "    print(f\"\\n\u26a0 Current directory: {os.getcwd()}\")\n",
    "    print(f\"\u26a0 If imports fail, make sure you've:\")\n",
    "    print(f\"   1. Run the git clone cell above\")\n",
    "    print(f\"   2. In Colab, the repo should be at: /content/tverskysimilaritygrad\")\n",
    "\n",
    "# Verify tverskycv can be found\n",
    "print(\"\\nVerifying tverskycv module location...\")\n",
    "try:\n",
    "    import tverskycv\n",
    "    print(f\"\u2713 tverskycv found at: {tverskycv.__file__}\")\n",
    "except ImportError:\n",
    "    print(\"\u2717 tverskycv module not found in Python path\")\n",
    "    print(f\"  sys.path entries: {sys.path[:10]}\")\n",
    "\n",
    "# Now import Tversky modules\n",
    "print(\"\\nImporting Tversky modules...\")\n",
    "try:\n",
    "    from tverskycv.models.backbones.shared_tversky import GlobalFeature\n",
    "    print(\"  \u2713 GlobalFeature imported\")\n",
    "    from tverskycv.models.backbones.tversky_reduce_backbone import (\n",
    "        TverskyReduceBackbone,\n",
    "        SharedTverskyCompact,\n",
    "        SharedTverskyInterpretable\n",
    "    )\n",
    "    print(\"  \u2713 TverskyReduceBackbone components imported\")\n",
    "    print(\"\\n\u2713 All imports successful!\")\n",
    "except ImportError as e:\n",
    "    print(f\"\\n\u2717 Import error: {e}\")\n",
    "    print(\"\\nTroubleshooting steps:\")\n",
    "    print(\"  1. Make sure you've run the git clone cell (Cell 2) above\")\n",
    "    print(\"  2. In Colab, uncomment and run: !git clone https://github.com/madch3m/tverskysimilaritygrad.git\")\n",
    "    print(\"  3. After cloning, the repo should be at: /content/tverskysimilaritygrad\")\n",
    "    print(f\"  4. Current working directory: {os.getcwd()}\")\n",
    "    print(f\"  5. Check if tverskycv exists: {os.path.exists('tverskycv')}\")\n",
    "    print(f\"  6. sys.path entries: {sys.path[:10]}\")\n",
    "    print(\"\\nIf still failing, try:\")\n",
    "    print(\"  - Restart the runtime after cloning\")\n",
    "    print(\"  - Manually add: sys.path.insert(0, '/content/tverskysimilaritygrad')\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Understanding TverskyReduceBackbone\n",
    "\n",
    "**TverskyReduceBackbone** combines CNN feature extraction with Tversky projection layers that use the **GlobalFeature bank** for parameter sharing:\n",
    "\n",
    "- **CNN Feature Extraction**: Standard convolutional layers extract spatial features from images\n",
    "- **Tversky Projection**: Compact or interpretable Tversky layers compute psychologically plausible similarity\n",
    "- **Feature Matrix Sharing**: Multiple layers share the same feature transformation matrix via GlobalFeature\n",
    "- **Parameter Reduction**: Significantly fewer trainable parameters compared to standard architectures\n",
    "\n",
    "### Key Benefits:\n",
    "- **Parameter Efficiency**: Shared features reduce total parameters by 50-90%\n",
    "- **Memory Efficient**: Lower memory footprint during training and inference\n",
    "- **Maintains Performance**: Shared features can still learn effective representations\n",
    "- **Two Variants**: Compact (minimal params) or Interpretable (visualization-friendly)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Load Fruits-360 Dataset\n",
    "\n",
    "Load the fruits-360 dataset from Hugging Face and prepare it for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Colab-specific optimizations\n",
    "os.environ['TOKENIZERS_PARALLELISM'] = 'false'\n",
    "os.environ['OMP_NUM_THREADS'] = '1'\n",
    "\n",
    "# Load fruits-360 dataset from Hugging Face\n",
    "print(\"Loading fruits-360 dataset...\")\n",
    "ds = load_dataset(\"PedroSampaio/fruits-360\")\n",
    "\n",
    "print(f\"\\nDataset splits: {list(ds.keys())}\")\n",
    "print(f\"Train samples: {len(ds['train'])}\")\n",
    "if 'test' in ds:\n",
    "    print(f\"Test samples: {len(ds['test'])}\")\n",
    "if 'validation' in ds:\n",
    "    print(f\"Validation samples: {len(ds['validation'])}\")\n",
    "\n",
    "# Get number of classes\n",
    "if 'train' in ds:\n",
    "    labels = ds['train']['label']\n",
    "    num_classes = len(set(labels))\n",
    "    print(f\"\\nNumber of classes: {num_classes}\")\n",
    "    print(f\"Sample labels: {sorted(set(labels))[:10]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data transformations\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((64, 64)),  # Resize to 64x64 for faster training\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # ImageNet normalization\n",
    "])\n",
    "\n",
    "def transform_dataset(examples):\n",
    "    \"\"\"Transform images in the dataset.\"\"\"\n",
    "    images = examples['image']\n",
    "    transformed_images = []\n",
    "    \n",
    "    for img in images:\n",
    "        if isinstance(img, Image.Image):\n",
    "            img_tensor = transform(img)\n",
    "        elif isinstance(img, np.ndarray):\n",
    "            img_pil = Image.fromarray(img)\n",
    "            img_tensor = transform(img_pil)\n",
    "        else:\n",
    "            img_tensor = torch.tensor(img) if not isinstance(img, torch.Tensor) else img\n",
    "        transformed_images.append(img_tensor.numpy())\n",
    "    \n",
    "    examples['image'] = transformed_images\n",
    "    return examples\n",
    "\n",
    "# Apply transformations\n",
    "print(\"Transforming dataset...\")\n",
    "train_data_transformed = ds['train'].map(\n",
    "    transform_dataset, \n",
    "    batched=True, \n",
    "    batch_size=100,\n",
    "    remove_columns=[col for col in ds['train'].column_names if col not in ['image', 'label']]\n",
    ")\n",
    "\n",
    "if 'test' in ds:\n",
    "    test_data_transformed = ds['test'].map(\n",
    "        transform_dataset,\n",
    "        batched=True,\n",
    "        batch_size=100,\n",
    "        remove_columns=[col for col in ds['test'].column_names if col not in ['image', 'label']]\n",
    "    )\n",
    "else:\n",
    "    # Use validation as test if test split doesn't exist\n",
    "    test_data_transformed = ds['validation'].map(\n",
    "        transform_dataset,\n",
    "        batched=True,\n",
    "        batch_size=100,\n",
    "        remove_columns=[col for col in ds['validation'].column_names if col not in ['image', 'label']]\n",
    "    )\n",
    "\n",
    "print(\"\u2713 Dataset transformation complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create PyTorch Dataset wrapper\n",
    "class Fruits360Dataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        item = self.data[idx]\n",
    "        image = item['image']\n",
    "        label = item['label']\n",
    "        \n",
    "        # Convert to tensor if needed\n",
    "        if not isinstance(image, torch.Tensor):\n",
    "            if isinstance(image, list):\n",
    "                image = torch.tensor(image)\n",
    "            elif isinstance(image, np.ndarray):\n",
    "                image = torch.from_numpy(image)\n",
    "            else:\n",
    "                image = torch.tensor(image)\n",
    "        \n",
    "        # Ensure image is in (C, H, W) format for backbone\n",
    "        if len(image.shape) == 1:\n",
    "            # Flattened image, reshape to (C, H, W)\n",
    "            image = image.view(3, 64, 64)\n",
    "        elif len(image.shape) == 3 and image.shape[0] != 3:\n",
    "            # Might be (H, W, C), transpose to (C, H, W)\n",
    "            if image.shape[2] == 3:\n",
    "                image = image.permute(2, 0, 1)\n",
    "        \n",
    "        return image, label\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = Fruits360Dataset(train_data_transformed)\n",
    "test_dataset = Fruits360Dataset(test_data_transformed)\n",
    "\n",
    "# Create data loaders\n",
    "batch_size = 32\n",
    "train_loader = DataLoader(\n",
    "    train_dataset, \n",
    "    batch_size=batch_size, \n",
    "    shuffle=True, \n",
    "    num_workers=0,  # Set to 0 for Colab compatibility\n",
    "    pin_memory=False\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    test_dataset, \n",
    "    batch_size=batch_size, \n",
    "    shuffle=False, \n",
    "    num_workers=0,\n",
    "    pin_memory=False\n",
    ")\n",
    "\n",
    "print(f\"\u2713 Data loaders created!\")\n",
    "print(f\"  Batch size: {batch_size}\")\n",
    "print(f\"  Train batches: {len(train_loader)}\")\n",
    "print(f\"  Test batches: {len(test_loader)}\")\n",
    "\n",
    "# Get image dimensions\n",
    "sample_image, _ = train_dataset[0]\n",
    "print(f\"  Image shape: {sample_image.shape}\")\n",
    "print(f\"  Image size: {sample_image.shape[1]}x{sample_image.shape[2]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Create Model with TverskyReduceBackbone\n",
    "\n",
    "Create a classification model using TverskyReduceBackbone with feature sharing.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FruitsClassifierWithTverskyBackbone(nn.Module):\n",
    "    \"\"\"Fruit classifier using TverskyReduceBackbone with feature sharing.\"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        num_classes,\n",
    "        in_channels=3,\n",
    "        img_size=64,\n",
    "        variant='compact',  # 'compact' or 'interpretable'\n",
    "        out_dim=128,\n",
    "        n_features=64,  # For interpretable variant\n",
    "        feature_key='fruits',\n",
    "        share_features=True,\n",
    "        alpha=1.0,\n",
    "        beta=1.0\n",
    "    ):\n",
    "        super().__init__()\n",
    "        \n",
    "        # TverskyReduceBackbone extracts features\n",
    "        self.backbone = TverskyReduceBackbone(\n",
    "            out_dim=out_dim,\n",
    "            in_channels=in_channels,\n",
    "            img_size=img_size,\n",
    "            variant=variant,\n",
    "            n_features=n_features,\n",
    "            feature_key=feature_key,\n",
    "            share_features=share_features,\n",
    "            alpha=alpha,\n",
    "            beta=beta\n",
    "        )\n",
    "        \n",
    "        # Classification head (simple linear layer)\n",
    "        self.classifier = nn.Linear(out_dim, num_classes)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Extract features using Tversky backbone\n",
    "        features = self.backbone(x)  # (B, out_dim)\n",
    "        \n",
    "        # Classify\n",
    "        logits = self.classifier(features)  # (B, num_classes)\n",
    "        return logits\n",
    "\n",
    "# Model configuration\n",
    "img_size = 64\n",
    "in_channels = 3\n",
    "variant = 'compact'  # Use 'compact' for efficiency or 'interpretable' for visualization\n",
    "out_dim = 128  # Output dimension from backbone\n",
    "feature_key = 'fruits_shared'  # Shared feature key\n",
    "\n",
    "# Clear GlobalFeature bank before creating model\n",
    "gf = GlobalFeature()\n",
    "gf.clear()\n",
    "\n",
    "# Create model with feature sharing\n",
    "model_shared = FruitsClassifierWithTverskyBackbone(\n",
    "    num_classes=num_classes,\n",
    "    in_channels=in_channels,\n",
    "    img_size=img_size,\n",
    "    variant=variant,\n",
    "    out_dim=out_dim,\n",
    "    feature_key=feature_key,\n",
    "    share_features=True  # Enable feature sharing\n",
    ")\n",
    "\n",
    "print(f\"\u2713 Model with TverskyReduceBackbone created!\")\n",
    "print(f\"  Variant: {variant}\")\n",
    "print(f\"  Image size: {img_size}x{img_size}\")\n",
    "print(f\"  Input channels: {in_channels}\")\n",
    "print(f\"  Backbone output dim: {out_dim}\")\n",
    "print(f\"  Number of classes: {num_classes}\")\n",
    "\n",
    "# Count parameters\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "params_shared = count_parameters(model_shared)\n",
    "print(f\"  Total parameters: {params_shared:,}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Parameter Efficiency Analysis\n",
    "\n",
    "Compare models with and without feature sharing to demonstrate parameter reduction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create model WITHOUT feature sharing for comparison\n",
    "gf.clear()  # Clear before creating non-shared model\n",
    "\n",
    "model_no_sharing = FruitsClassifierWithTverskyBackbone(\n",
    "    num_classes=num_classes,\n",
    "    in_channels=in_channels,\n",
    "    img_size=img_size,\n",
    "    variant=variant,\n",
    "    out_dim=out_dim,\n",
    "    feature_key='fruits_no_sharing',\n",
    "    share_features=False  # Disable feature sharing\n",
    ")\n",
    "\n",
    "params_no_sharing = count_parameters(model_no_sharing)\n",
    "\n",
    "# Calculate reduction\n",
    "reduction = ((params_no_sharing - params_shared) / params_no_sharing) * 100\n",
    "params_saved = params_no_sharing - params_shared\n",
    "\n",
    "print(\"Parameter Efficiency Comparison:\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"Model with feature sharing:    {params_shared:,} parameters\")\n",
    "print(f\"Model without feature sharing: {params_no_sharing:,} parameters\")\n",
    "print(f\"Parameters saved:              {params_saved:,}\")\n",
    "print(f\"Reduction:                      {reduction:.2f}%\")\n",
    "print(f\"Efficiency ratio:               {params_no_sharing / params_shared:.2f}x fewer parameters\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Analyze shared features\n",
    "print(\"\\nShared Feature Analysis:\")\n",
    "print(\"=\" * 70)\n",
    "# Re-create shared model to see features\n",
    "gf.clear()\n",
    "model_shared_check = FruitsClassifierWithTverskyBackbone(\n",
    "    num_classes=num_classes,\n",
    "    in_channels=in_channels,\n",
    "    img_size=img_size,\n",
    "    variant=variant,\n",
    "    out_dim=out_dim,\n",
    "    feature_key=feature_key,\n",
    "    share_features=True\n",
    ")\n",
    "shared_features = gf._feature_matrices\n",
    "total_shared_params = 0\n",
    "\n",
    "for key, value in shared_features.items():\n",
    "    if isinstance(value, nn.Parameter):\n",
    "        param_count = value.numel()\n",
    "        total_shared_params += param_count\n",
    "        print(f\"  {key}: {param_count:,} parameters\")\n",
    "    elif isinstance(value, dict):\n",
    "        param_count = sum(p.numel() for p in value.values() if isinstance(p, nn.Parameter))\n",
    "        total_shared_params += param_count\n",
    "        print(f\"  {key}: {param_count:,} parameters (Tversky params)\")\n",
    "\n",
    "print(f\"\\nTotal shared parameters: {total_shared_params:,}\")\n",
    "if params_shared > 0:\n",
    "    print(f\"Shared percentage: {(total_shared_params / params_shared * 100):.2f}%\")\n",
    "print(\"=\" * 70)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare with Traditional CNN\n",
    "\n",
    "Now let's compare with a traditional convolutional neural network to see the parameter difference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Traditional CNN Model for comparison\n",
    "class TraditionalCNN(nn.Module):\n",
    "    \"\"\"Traditional convolutional neural network for fruit classification.\"\"\"\n",
    "    \n",
    "    def __init__(self, num_classes, img_size=64):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Convolutional layers\n",
    "        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
    "        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n",
    "        \n",
    "        # Pooling layers\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        \n",
    "        # Calculate flattened size after convolutions\n",
    "        # After 3 pooling operations: 64 -> 32 -> 16 -> 8\n",
    "        self.flattened_size = 128 * 8 * 8\n",
    "        \n",
    "        # Fully connected layers\n",
    "        self.fc1 = nn.Linear(self.flattened_size, 512)\n",
    "        self.fc2 = nn.Linear(512, 256)\n",
    "        self.fc3 = nn.Linear(256, 128)\n",
    "        self.fc4 = nn.Linear(128, num_classes)\n",
    "        \n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Input should be in (B, C, H, W) format\n",
    "        batch_size = x.size(0)\n",
    "        \n",
    "        # Handle both (B, C, H, W) and flattened formats\n",
    "        if len(x.shape) == 2:\n",
    "            # Flattened input, reshape to (B, C, H, W)\n",
    "            x = x.view(batch_size, 3, 64, 64)\n",
    "        \n",
    "        # Assuming input is flattened (B, 64*64*3) = (B, 12288)\n",
    "        batch_size = x.size(0)\n",
    "        x = x.view(batch_size, 3, 64, 64)\n",
    "        \n",
    "        # Convolutional layers with pooling\n",
    "        x = self.pool(F.relu(self.conv1(x)))  # 64x64 -> 32x32\n",
    "        x = self.pool(F.relu(self.conv2(x)))  # 32x32 -> 16x16\n",
    "        x = self.pool(F.relu(self.conv3(x)))  # 16x16 -> 8x8\n",
    "        \n",
    "        # Flatten\n",
    "        x = x.view(batch_size, -1)\n",
    "        \n",
    "        # Fully connected layers\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.dropout(x)\n",
    "        x = F.relu(self.fc3(x))\n",
    "        x = self.fc4(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "# Create traditional CNN model\n",
    "model_cnn = TraditionalCNN(num_classes=num_classes, img_size=64)\n",
    "params_cnn = count_parameters(model_cnn)\n",
    "\n",
    "print(\"Traditional CNN Model:\")\n",
    "print(f\"  Total parameters: {params_cnn:,}\")\n",
    "print(f\"  Architecture: Conv2d layers + MaxPool + Fully Connected layers\")\n",
    "print(f\"  Conv layers: 3x3 convs with 32, 64, 128 channels\")\n",
    "print(f\"  FC layers: 512 -> 256 -> 128 -> {num_classes}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Three-way Parameter Comparison\n",
    "print(\"=\" * 80)\n",
    "print(\"COMPREHENSIVE PARAMETER COMPARISON\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"\\n1. TverskyReduceBackbone with Feature Sharing:\")\n",
    "print(f\"   Parameters: {params_shared:,} ({params_shared/1e6:.2f}M)\")\n",
    "print(f\"   Architecture: CNN + TverskyCompact/Interpretable with GlobalFeature bank\")\n",
    "print(f\"   Feature: Parameter sharing across layers via GlobalFeature\")\n",
    "\n",
    "print(f\"\\n2. TverskyReduceBackbone without Feature Sharing:\")\n",
    "print(f\"   Parameters: {params_no_sharing:,} ({params_no_sharing/1e6:.2f}M)\")\n",
    "print(f\"   Architecture: CNN + TverskyCompact/Interpretable without sharing\")\n",
    "print(f\"   Feature: Each layer has its own features\")\n",
    "\n",
    "print(f\"\\n3. Traditional CNN Model:\")\n",
    "print(f\"   Parameters: {params_cnn:,} ({params_cnn/1e6:.2f}M)\")\n",
    "print(f\"   Architecture: Conv2d + MaxPool + Linear layers\")\n",
    "print(f\"   Feature: Standard convolutional neural network\")\n",
    "\n",
    "# Calculate reductions\n",
    "reduction_vs_no_sharing = ((params_no_sharing - params_shared) / params_no_sharing) * 100\n",
    "reduction_vs_cnn = ((params_cnn - params_shared) / params_cnn) * 100\n",
    "reduction_cnn_vs_no_sharing = ((params_no_sharing - params_cnn) / params_no_sharing) * 100\n",
    "\n",
    "print(f\"\\n\" + \"=\" * 80)\n",
    "print(\"PARAMETER REDUCTION ANALYSIS\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"\\nFeature Sharing vs No Sharing:\")\n",
    "print(f\"  Reduction: {reduction_vs_no_sharing:.2f}%\")\n",
    "print(f\"  Parameters saved: {params_no_sharing - params_shared:,}\")\n",
    "print(f\"  Efficiency: {params_no_sharing / params_shared:.2f}x fewer parameters\")\n",
    "\n",
    "print(f\"\\nFeature Sharing vs Traditional CNN:\")\n",
    "print(f\"  Reduction: {reduction_vs_cnn:.2f}%\")\n",
    "print(f\"  Parameters saved: {params_cnn - params_shared:,}\")\n",
    "print(f\"  Efficiency: {params_cnn / params_shared:.2f}x fewer parameters\")\n",
    "\n",
    "print(f\"\\nTraditional CNN vs No Sharing:\")\n",
    "if params_cnn < params_no_sharing:\n",
    "    print(f\"  CNN has {params_no_sharing - params_cnn:,} fewer parameters ({reduction_cnn_vs_no_sharing:.2f}% reduction)\")\n",
    "else:\n",
    "    print(f\"  No Sharing model has {params_cnn - params_no_sharing:,} fewer parameters\")\n",
    "\n",
    "print(\"=\" * 80)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize three-way parameter comparison\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Bar chart comparison (all three models)\n",
    "ax1 = axes[0]\n",
    "models = ['Tversky\\n(Shared)', 'Tversky\\n(No Share)', 'Traditional\\nCNN']\n",
    "params = [params_shared, params_no_sharing, params_cnn]\n",
    "colors = ['#2ecc71', '#e74c3c', '#3498db']\n",
    "\n",
    "bars = ax1.bar(models, params, color=colors, alpha=0.7, edgecolor='black', linewidth=2)\n",
    "ax1.set_ylabel('Number of Parameters', fontsize=12, fontweight='bold')\n",
    "ax1.set_title('Parameter Count: All Model Types Comparison', fontsize=14, fontweight='bold')\n",
    "ax1.grid(axis='y', alpha=0.3, linestyle='--')\n",
    "\n",
    "# Add value labels\n",
    "for bar, param in zip(bars, params):\n",
    "    height = bar.get_height()\n",
    "    ax1.text(bar.get_x() + bar.get_width()/2., height,\n",
    "            f'{param:,}\\n({param/1e6:.2f}M)',\n",
    "            ha='center', va='bottom', fontsize=9, fontweight='bold')\n",
    "\n",
    "# Reduction percentages comparison\n",
    "ax2 = axes[1]\n",
    "reductions = [reduction_vs_no_sharing, reduction_vs_cnn]\n",
    "reduction_labels = ['vs No Sharing', 'vs Traditional CNN']\n",
    "colors_reduction = ['#e74c3c', '#3498db']\n",
    "\n",
    "bars2 = ax2.bar(reduction_labels, reductions, color=colors_reduction, alpha=0.7, edgecolor='black', linewidth=2)\n",
    "ax2.set_ylabel('Parameter Reduction (%)', fontsize=12, fontweight='bold')\n",
    "ax2.set_title('TverskyReduceBackbone: Parameter Reduction', fontsize=14, fontweight='bold')\n",
    "ax2.grid(axis='y', alpha=0.3, linestyle='--')\n",
    "\n",
    "# Add value labels\n",
    "for bar, reduction in zip(bars2, reductions):\n",
    "    height = bar.get_height()\n",
    "    ax2.text(bar.get_x() + bar.get_width()/2., height,\n",
    "            f'{reduction:.2f}%',\n",
    "            ha='center', va='bottom', fontsize=11, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Summary table\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"SUMMARY TABLE\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"{'Model Type':<30} {'Parameters':<20} {'Reduction vs CNN':<20}\")\n",
    "print(\"-\" * 80)\n",
    "print(f\"{'TverskyReduce (Shared)':<30} {params_shared:>18,} ({params_shared/1e6:>5.2f}M) {reduction_vs_cnn:>18.2f}%\")\n",
    "print(f\"{'TverskyReduce (No Share)':<30} {params_no_sharing:>18,} ({params_no_sharing/1e6:>5.2f}M) {'N/A':>20}\")\n",
    "print(f\"{'Traditional CNN':<30} {params_cnn:>18,} ({params_cnn/1e6:>5.2f}M) {'Baseline':>20}\")\n",
    "print(\"=\" * 80)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Training Setup\n",
    "\n",
    "Set up training configuration and utilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training configuration\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Use the shared model for training\n",
    "gf.clear()\n",
    "model = FruitsClassifierWithTverskyBackbone(\n",
    "    num_classes=num_classes,\n",
    "    in_channels=in_channels,\n",
    "    img_size=img_size,\n",
    "    variant=variant,\n",
    "    out_dim=out_dim,\n",
    "    feature_key=feature_key,\n",
    "    share_features=True\n",
    ").to(device)\n",
    "\n",
    "# Loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.5)\n",
    "\n",
    "print(f\"\\n\u2713 Training setup complete!\")\n",
    "print(f\"  Model: TverskyReduceBackbone ({variant} variant)\")\n",
    "print(f\"  Loss function: CrossEntropyLoss\")\n",
    "print(f\"  Optimizer: Adam (lr=0.001)\")\n",
    "print(f\"  Scheduler: StepLR (step_size=5, gamma=0.5)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Training Loop\n",
    "\n",
    "Train the model with feature sharing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training parameters\n",
    "num_epochs = 10\n",
    "\n",
    "# Track metrics\n",
    "train_losses = []\n",
    "train_accuracies = []\n",
    "val_losses = []\n",
    "val_accuracies = []\n",
    "\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(f\"Starting Training for {num_epochs} epochs\")\n",
    "print(f\"{'='*70}\\n\")\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # Training phase\n",
    "    model.train()\n",
    "    epoch_train_loss = 0.0\n",
    "    epoch_train_correct = 0\n",
    "    epoch_train_total = 0\n",
    "    \n",
    "    for batch_idx, (images, labels) in enumerate(train_loader):\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        optimizer.zero_grad()\n",
    "        logits = model(images)\n",
    "        loss = criterion(logits, labels)\n",
    "        \n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Statistics\n",
    "        epoch_train_loss += loss.item()\n",
    "        preds = torch.argmax(logits, dim=-1)\n",
    "        epoch_train_correct += (preds == labels).sum().item()\n",
    "        epoch_train_total += labels.size(0)\n",
    "        \n",
    "        # Print progress\n",
    "        if (batch_idx + 1) % 100 == 0:\n",
    "            current_loss = epoch_train_loss / (batch_idx + 1)\n",
    "            current_acc = epoch_train_correct / epoch_train_total\n",
    "            print(f\"  Batch {batch_idx+1}/{len(train_loader)}: Loss={current_loss:.4f}, Acc={current_acc:.4f}\")\n",
    "    \n",
    "    # Calculate average training metrics\n",
    "    train_loss = epoch_train_loss / len(train_loader)\n",
    "    train_acc = epoch_train_correct / epoch_train_total\n",
    "    \n",
    "    # Validation phase\n",
    "    model.eval()\n",
    "    epoch_val_loss = 0.0\n",
    "    epoch_val_correct = 0\n",
    "    epoch_val_total = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for images, labels in test_loader:\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            logits = model(images)\n",
    "            loss = criterion(logits, labels)\n",
    "            \n",
    "            epoch_val_loss += loss.item()\n",
    "            preds = torch.argmax(logits, dim=-1)\n",
    "            epoch_val_correct += (preds == labels).sum().item()\n",
    "            epoch_val_total += labels.size(0)\n",
    "    \n",
    "    # Calculate average validation metrics\n",
    "    val_loss = epoch_val_loss / len(test_loader)\n",
    "    val_acc = epoch_val_correct / epoch_val_total\n",
    "    \n",
    "    # Store metrics\n",
    "    train_losses.append(train_loss)\n",
    "    train_accuracies.append(train_acc)\n",
    "    val_losses.append(val_loss)\n",
    "    val_accuracies.append(val_acc)\n",
    "    \n",
    "    # Update learning rate\n",
    "    scheduler.step()\n",
    "    \n",
    "    # Print epoch summary\n",
    "    print(f\"\\nEpoch {epoch+1}/{num_epochs}:\")\n",
    "    print(f\"  Train - Loss: {train_loss:.4f}, Accuracy: {train_acc:.4f}\")\n",
    "    print(f\"  Val   - Loss: {val_loss:.4f}, Accuracy: {val_acc:.4f}\")\n",
    "    print(f\"  Learning Rate: {scheduler.get_last_lr()[0]:.6f}\")\n",
    "    print(f\"  {'-'*60}\")\n",
    "\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(f\"Training Complete!\")\n",
    "print(f\"{'='*70}\")\n",
    "print(f\"Final Training Accuracy: {train_accuracies[-1]:.4f}\")\n",
    "print(f\"Final Validation Accuracy: {val_accuracies[-1]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Visualize Training Results\n",
    "\n",
    "Plot training curves and analyze model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training curves\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Loss plot\n",
    "axes[0].plot(train_losses, label='Train Loss', marker='o', linewidth=2)\n",
    "axes[0].plot(val_losses, label='Val Loss', marker='s', linewidth=2)\n",
    "axes[0].set_xlabel('Epoch', fontsize=12)\n",
    "axes[0].set_ylabel('Loss', fontsize=12)\n",
    "axes[0].set_title('Training and Validation Loss', fontsize=14, fontweight='bold')\n",
    "axes[0].legend(fontsize=11)\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Accuracy plot\n",
    "axes[1].plot(train_accuracies, label='Train Acc', marker='o', linewidth=2)\n",
    "axes[1].plot(val_accuracies, label='Val Acc', marker='s', linewidth=2)\n",
    "axes[1].set_xlabel('Epoch', fontsize=12)\n",
    "axes[1].set_ylabel('Accuracy', fontsize=12)\n",
    "axes[1].set_title('Training and Validation Accuracy', fontsize=14, fontweight='bold')\n",
    "axes[1].legend(fontsize=11)\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Final Evaluation\n",
    "\n",
    "Evaluate the trained model on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final test evaluation\n",
    "model.eval()\n",
    "test_loss = 0.0\n",
    "test_correct = 0\n",
    "test_total = 0\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "\n",
    "print(\"Evaluating on test set...\")\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_loader:\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        logits = model(images)\n",
    "        loss = criterion(logits, labels)\n",
    "        \n",
    "        test_loss += loss.item()\n",
    "        preds = torch.argmax(logits, dim=-1)\n",
    "        test_correct += (preds == labels).sum().item()\n",
    "        test_total += labels.size(0)\n",
    "        \n",
    "        all_preds.extend(preds.cpu().numpy())\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "test_loss = test_loss / len(test_loader)\n",
    "test_acc = test_correct / test_total\n",
    "\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(f\"Final Test Results\")\n",
    "print(f\"{'='*70}\")\n",
    "print(f\"Test Loss: {test_loss:.4f}\")\n",
    "print(f\"Test Accuracy: {test_acc:.4f} ({test_correct}/{test_total})\")\n",
    "print(f\"{'='*70}\")\n",
    "\n",
    "# Per-class accuracy (top 10)\n",
    "correct_by_class = Counter()\n",
    "total_by_class = Counter()\n",
    "\n",
    "for pred, label in zip(all_preds, all_labels):\n",
    "    total_by_class[label] += 1\n",
    "    if pred == label:\n",
    "        correct_by_class[label] += 1\n",
    "\n",
    "print(f\"\\nPer-class accuracy (top 10 classes):\")\n",
    "class_accuracies = {cls: correct_by_class[cls] / total_by_class[cls] \n",
    "                    for cls in sorted(total_by_class.keys())[:10]}\n",
    "for cls, acc in sorted(class_accuracies.items(), key=lambda x: x[1], reverse=True):\n",
    "    print(f\"  Class {cls}: {acc:.4f} ({correct_by_class[cls]}/{total_by_class[cls]})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Summary\n",
    "\n",
    "### Key Takeaways:\n",
    "\n",
    "1. **Parameter Efficiency**: TverskyReduceBackbone with feature sharing significantly reduces the number of trainable parameters\n",
    "2. **Performance**: The model maintains good classification performance despite fewer parameters\n",
    "3. **Memory Efficiency**: Lower memory footprint enables training on resource-constrained devices\n",
    "4. **Scalability**: Parameter reduction becomes more significant with larger models\n",
    "\n",
    "### TverskyReduceBackbone Benefits:\n",
    "\n",
    "- **CNN + Tversky Architecture**: Combines convolutional feature extraction with Tversky similarity\n",
    "- **Shared Feature Matrices**: Multiple layers share the same feature transformation via GlobalFeature\n",
    "- **Shared Tversky Parameters**: Alpha and beta parameters are shared across layers\n",
    "- **Two Variants**: Compact (efficient) or Interpretable (visualizable)\n",
    "- **GlobalFeature Bank**: Centralized parameter storage for efficient sharing\n",
    "\n",
    "### Next Steps:\n",
    "\n",
    "- Experiment with different variants (compact vs interpretable)\n",
    "- Try different backbone output dimensions\n",
    "- Compare with standard CNN architectures\n",
    "- Fine-tune Tversky parameters (alpha, beta) for better performance\n",
    "- Visualize learned prototypes (with interpretable variant)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}